{
  "hash": "100b19ae787cb112e993adbf6ae03fb8",
  "result": {
    "markdown": "---\ntitle: \"Statistical models\"\neditor: visual\nbibliography: references.bib\n---\n\n\n\n## Example data\n\nWe will continue using the same data on white mold epidemics in soybean seen in previous chapter.\n\n\n::: {.cell hash='yieldloss-regression-models_cache/html/unnamed-chunk-1_d5435cebd956d48fa46913651d8d8879'}\n\n```{.r .cell-code}\nlibrary(r4pde)\nwm <- WhiteMoldSoybean\n```\n:::\n\n\nFirst, let's work with data from a single trial (trial 1).\n\n\n::: {.cell hash='yieldloss-regression-models_cache/html/unnamed-chunk-2_622b22c7e5b88ec048d405dcf839495d'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nwm1 <- wm |> \n  dplyr::select(study, inc, yld) |> \n  filter(study %in% c(1)) \nhead(wm1, 13)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 13 × 3\n   study   inc   yld\n   <dbl> <dbl> <dbl>\n 1     1    76  2265\n 2     1    53  2618\n 3     1    42  2554\n 4     1    37  2632\n 5     1    29  2820\n 6     1    42  2799\n 7     1    55  2503\n 8     1    40  2967\n 9     1    26  2965\n10     1    18  3088\n11     1    27  3044\n12     1    28  2925\n13     1    36  2867\n```\n:::\n:::\n\n\n## Damage curves\n\nWe can now plot the damage curve by relating soybean yield to white mold incidence.\n\n\n::: {.cell hash='yieldloss-regression-models_cache/html/unnamed-chunk-3_9cfe2bcf91bf7e98d0c270808c32d702'}\n\n```{.r .cell-code}\nwm1 |> \n  ggplot(aes(inc, yld))+\n  theme_r4pde(font_size = 14)+\n  geom_point(size = 2)+\n  geom_smooth(method = \"lm\", se = F, \n              color = \"black\", fullrange = T)+\n  ylim(1800, 3500)+\n  labs(x = \"White mold incidence (%)\",\n       y = \"Soybean yield (kg/ha)\")\n```\n\n::: {.cell-output-display}\n![](yieldloss-regression-models_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n## Linear regression\n\nAssuming a linear relationship between the variables, we can employ a linear regression model. From this model, the intercept provides an indication of the attainable yield (the yield when there is no disease). Conversely, the slope represents the rate at which the yield decreases for every one percentage point increase in disease incidence.\n\n\n::: {.cell hash='yieldloss-regression-models_cache/html/unnamed-chunk-4_ba2f9630c7b2c62c96f753ce4d53dec8'}\n\n```{.r .cell-code}\nlm1 <-  lm(yld ~ inc, data = wm1) \njtools::summ(lm1)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover table-condensed table-responsive\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n<tbody>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> Observations </td>\n   <td style=\"text-align:right;\"> 13 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> Dependent variable </td>\n   <td style=\"text-align:right;\"> yld </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> Type </td>\n   <td style=\"text-align:right;\"> OLS linear regression </td>\n  </tr>\n</tbody>\n</table> <table class=\"table table-striped table-hover table-condensed table-responsive\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n<tbody>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> F(1,11) </td>\n   <td style=\"text-align:right;\"> 46.86 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> R² </td>\n   <td style=\"text-align:right;\"> 0.81 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> Adj. R² </td>\n   <td style=\"text-align:right;\"> 0.79 </td>\n  </tr>\n</tbody>\n</table> <table class=\"table table-striped table-hover table-condensed table-responsive\" style=\"width: auto !important; margin-left: auto; margin-right: auto;border-bottom: 0;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:right;\"> Est. </th>\n   <th style=\"text-align:right;\"> S.E. </th>\n   <th style=\"text-align:right;\"> t val. </th>\n   <th style=\"text-align:right;\"> p </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> (Intercept) </td>\n   <td style=\"text-align:right;\"> 3329.14 </td>\n   <td style=\"text-align:right;\"> 86.84 </td>\n   <td style=\"text-align:right;\"> 38.33 </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> inc </td>\n   <td style=\"text-align:right;\"> -14.21 </td>\n   <td style=\"text-align:right;\"> 2.08 </td>\n   <td style=\"text-align:right;\"> -6.85 </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n  </tr>\n</tbody>\n<tfoot><tr><td style=\"padding: 0; \" colspan=\"100%\">\n<sup></sup> Standard errors: OLS</td></tr></tfoot>\n</table>\n`````\n:::\n:::\n\n\nThe model explains a statistically significant and substantial proportion of variance (R2 = 0.81, F(1, 11) = 46.86, p \\< .001, adj. R2 = 0.79). The model's intercept, corresponding to inc = 0, is at 3329.14 (95% CI \\[3138.00, 3520.28\\], t(11) = 38.33, p \\< .001). The effect of inc is statistically significant and negative (beta = -14.21, 95% CI \\[-18.78, -9.64\\], t(11) = -6.85, p \\< .001; Std. beta = -0.90, 95% CI \\[-1.19, -0.61\\]). In other words, 14.21 kg is lost for each unitary increase in incidence, given the attainable yield of 3,329.14 kg.\n\n## Damage coefficients\n\nDamage curves offer a visual representation of how various factors, in this case plant diseases, can impact a given system. When we want to normalize these effects to better compare across different systems or conditions, it's useful to express these curves in relative terms rather than absolute ones. To achieve this, we can adjust the derived slope by dividing it by the intercept. This step essentially scales the rate of damage in relation to the baseline or the starting point (when there's no damage). By subsequently multiplying the result by 100, we convert this value into a percentage. This percentage is termed the \"relative damage coefficient\". What makes this coefficient particularly useful is its ability to standardize the measurement of damage, facilitating comparisons across diverse pathosystems.\n\nTwo plots can be used, one that shows the effect of the disease on the relative yield and the other that shows the effect on yield loss (which in this case represents a positive slope). Both representations can be found in the literature.\n\n\n::: {.cell hash='yieldloss-regression-models_cache/html/unnamed-chunk-5_1df9f31e99883a77f265df99621f954f'}\n\n```{.r .cell-code}\ndc <- (lm1$coefficients[2]/lm1$coefficients[1])*100\ndc \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      inc \n-0.426775 \n```\n:::\n\n```{.r .cell-code}\n# Plot for the relative damage curve\nx = seq(0,100,0.1)\ny = seq(0,100,0.1)\ndat <- data.frame(x,y)\n\np1 <- dat |> \n  ggplot(aes(x,y))+\n  theme_r4pde(font_size = 14)+\n  geom_point(color = \"NA\")+  \n  scale_y_continuous(expand = c(0,0))+\n  scale_x_continuous(expand = c(0,0))+\n  geom_abline(aes(intercept = 100, slope = dc))+\n  labs(x = \"Incidence (%)\", y = \"Yield (%)\")+\n  annotate(geom = \"text\", x = 60, y = 60, label = \"DC = -0.42\")\n\np1 \n```\n\n::: {.cell-output-display}\n![](yieldloss-regression-models_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Plot for the relative yield decrease\ndc2 <- (-lm1$coefficients[2]/lm1$coefficients[1])*100\ndc2 \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     inc \n0.426775 \n```\n:::\n\n```{.r .cell-code}\ndat <- data.frame(x,y)\np2 <- dat |> \n  ggplot(aes(x,y))+\n  theme_r4pde(font_size = 14)+\n  geom_point(color = \"NA\")+  \n  scale_y_continuous(expand = c(0,0))+\n  scale_x_continuous(expand = c(0,0))+\n  geom_abline(aes(intercept = 0, slope = dc2))+\n  labs(x = \"Incidence (%)\", y = \"Yield loss (%)\")+\n  annotate(geom = \"text\", x = 60, y = 60, label = \"DC = 0.42\")\np2 \n```\n\n::: {.cell-output-display}\n![](yieldloss-regression-models_files/figure-html/unnamed-chunk-5-2.png){width=672}\n:::\n:::\n\n\n## Multiple trials\n\n### Introduction\n\nWhen managing data sourced from multiple trials, a naive and straightforward approach is to pool all the data and fit a \"global\" linear regression. This tactic, while efficient, operates under the assumption that the different trials share common underlying characteristics, which might not always be the case.\n\nAnother simplistic methodology entails running independent linear regressions for each trial and subsequently averaging the intercepts and slopes. While this provides a glimpse into the general behavior of the data, it potentially sidesteps important variations that exist between individual trials. This variation is crucial, as different trials could have unique environments, treatments, or experimental conditions, all of which can influence results.\n\nNeglecting these variations can mask important nuances and lead to overgeneralized or inaccurate conclusions. In order to accommodate the inherent variability and structure present in multitrial data, researchers often turn to more refined analytical frameworks. Two such frameworks stand out in their ability to provide a nuanced view:\n\n1.  **Meta-Analytic modeling:** This approach synthesizes results across multiple studies to arrive at a more comprehensive understanding. In the context of linear regressions across multiple trials, the coefficients (both intercepts and slopes) from each trial can be treated as effect sizes. The standard error of these coefficients, which reflects the precision of the estimate, can then be used to weight each coefficient, ensuring that more reliable estimates have greater influence on the overall mean. This method can also provide insights into heterogeneity across trials, and if required, moderators can be explored to account for this variability.\n\n2.  **Random Coefficients (Mixed Effects) modelling:** This approach allows the intercepts and slopes to vary across different trials, treating them as random effects. By modeling the coefficients as random effects, we're assuming they come from a distribution, and we aim to estimate the parameters of this distribution. This structure acknowledges that variability exists between different trials and allows for the sharing of information across trials, thereby improving the estimation.\n\nBoth methods have their strengths and are appropriate under different circumstances. The choice largely depends on the research question, data structure, and the assumptions one is willing to make.\n\n### Global regression\n\nLet's begin by fitting a \"global regression\", but we might want to first inspect the damage curve visually.\n\n\n::: {.cell hash='yieldloss-regression-models_cache/html/unnamed-chunk-6_1bc96c1ec9b913a9bd78518d550daafb'}\n\n```{.r .cell-code}\nggplot(wm, aes(inc, yld))+\n      theme_r4pde(font_size = 12)+\n       geom_point(shape = 1)+\n       stat_smooth(method = lm, fullrange=TRUE, se = F, col = \"black\")+\n       ylab(\"Yield (kg/ha)\")+\n       xlab(\"White mold incidence (%)\")\n```\n\n::: {.cell-output-display}\n![](yieldloss-regression-models_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nA \"global\" regression can be performed using:\n\n\n::: {.cell hash='yieldloss-regression-models_cache/html/unnamed-chunk-7_a996718baed83d155a5944126b027c70'}\n\n```{.r .cell-code}\nlibrary(broom)\nfit_global <- wm%>%\n  do(tidy(lm(.$yld ~ .$inc), conf.int=TRUE))\nfit_global\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 7\n  term        estimate std.error statistic   p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)  3300.       56.5      58.5  5.14e-192   3189.    3411.  \n2 .$inc          -9.26      2.11     -4.39 1.45e-  5    -13.4     -5.12\n```\n:::\n:::\n\n\nThe global intercept and slope were estimated as 3,299 kg/ha and -9.26 kg/p.p. (percent point), respectively.\n\n### Individual regressions\n\nNow we can fit separate regressions for each trial. Let's visualize first.\n\n\n::: {.cell hash='yieldloss-regression-models_cache/html/unnamed-chunk-8_4a1e9ab8c5a29348b19310b1067a3820'}\n\n```{.r .cell-code}\nggplot(wm, aes(inc, yld))+\n      theme_r4pde(font_size = 12)+\n       geom_point(shape = 1)+\n       stat_smooth(method = lm, fullrange=TRUE, se = F, col = \"black\")+\n       ylab(\"Yield (kg/ha)\")+\n       xlab(\"White mold incidence (%)\")+\n      facet_wrap(~ study, ncol = 7, scales = \"fixed\") \n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](yieldloss-regression-models_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nTo fit separate regression lines for each study and extract the coefficients, we can use the **`group_by()`** function along with **`do()`**.\n\n\n::: {.cell hash='yieldloss-regression-models_cache/html/unnamed-chunk-9_4816a8350c399e6852bde8f8dd59f739'}\n\n```{.r .cell-code}\nfit_all <- wm%>%\n  group_by(study) |> \n  do(tidy(lm(.$yld ~ .$inc), conf.int=TRUE))\nfit_all\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 70 × 8\n# Groups:   study [35]\n   study term        estimate std.error statistic  p.value conf.low conf.high\n   <dbl> <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n 1     1 (Intercept)  3329.       86.8      38.3  4.60e-13   3138.    3520.  \n 2     1 .$inc         -14.2       2.08     -6.85 2.78e- 5    -18.8     -9.64\n 3     2 (Intercept)  2682.       48.6      55.2  8.55e-15   2575.    2789.  \n 4     2 .$inc          -6.93      1.49     -4.66 6.89e- 4    -10.2     -3.66\n 5     3 (Intercept)  4017.       61.6      65.2  1.37e-15   3882.    4153.  \n 6     3 .$inc         -18.6       1.71    -10.9  3.11e- 7    -22.4    -14.9 \n 7     4 (Intercept)  2814.      151.       18.6  1.15e- 9   2481.    3147.  \n 8     4 .$inc         -43.5      16.8      -2.58 2.56e- 2    -80.5     -6.38\n 9     5 (Intercept)  3317.      234.       14.2  2.07e- 8   2802.    3832.  \n10     5 .$inc         -21.2       5.69     -3.72 3.36e- 3    -33.7     -8.67\n# ℹ 60 more rows\n```\n:::\n:::\n\n\nWith this code above, the data is first grouped by the `study` column. Then, for each study, a linear regression is fitted with `yld` as the response variable and `inc` as the predictor. The `tidy` function from the `broom` package is then used to extract the coefficients and confidence intervals for each regression line. The resulting output should give you a tidy dataframe with coefficients for each trial.\n\nNow we can plot the distributions of each coefficient.\n\n\n::: {.cell hash='yieldloss-regression-models_cache/html/unnamed-chunk-10_efcc037e636a275387730889ca0dc3db'}\n\n```{.r .cell-code}\np3 <- fit_all |> \n  filter(term == \"(Intercept)\") |> \n  ggplot(aes(x = estimate))+\n  geom_histogram(bins = 8, color = \"white\", fill = \"gray50\")+\n  theme_r4pde()+\n  labs(x = \"Intercept\", y = \"Frequency\")\n\np4 <- fit_all |> \n  filter(term == \".$inc\") |> \n  ggplot(aes(x = estimate))+\n  geom_histogram(bins = 8, color = \"white\", fill = \"gray50\")+\n  theme_r4pde()+\n  labs(x = \"Slope\", y = \"Frequency\")\n\n\nlibrary(patchwork)\np3 | p4\n```\n\n::: {.cell-output-display}\n![](yieldloss-regression-models_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nLet's summarize the data on the slopes and intercept to compare with the global regression approach.\n\n\n::: {.cell hash='yieldloss-regression-models_cache/html/unnamed-chunk-11_13ffa49492992601c939b6860751196b'}\n\n```{.r .cell-code}\nfit_all |> \n  filter(term == \"(Intercept)\") |>\n  ungroup() |> \n  dplyr::select(estimate) |> \n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    estimate   \n Min.   :1760  \n 1st Qu.:2863  \n Median :3329  \n Mean   :3482  \n 3rd Qu.:4080  \n Max.   :4923  \n```\n:::\n\n```{.r .cell-code}\nfit_all |> \n  filter(term == \".$inc\") |>\n  ungroup() |> \n  dplyr::select(estimate) |> \n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    estimate      \n Min.   :-43.455  \n 1st Qu.:-27.676  \n Median :-16.926  \n Mean   :-19.529  \n 3rd Qu.:-13.054  \n Max.   :  2.712  \n```\n:::\n:::\n\n\nNote that the values of the coefficients are very different from those obtained with the global regression model.\n\n### Meta-analysis\n\nHere the objective is to combine the estimates from multiple studies or trials into a single overall estimate using a random-effects meta-analysis. The goal is to capture both the central tendency (i.e., the overall effect) and the variability (heterogeneity) among those individual estimates.\n\n\n::: {.cell hash='yieldloss-regression-models_cache/html/unnamed-chunk-12_6bf7581d03bdd6d87bd096dad52d5b1f'}\n\n```{.r .cell-code}\n# data preparation\nIntercepts <- fit_all |> \n  filter(term == \"(Intercept)\")\n\nSlopes <-  fit_all |> \n  filter(term == \".$inc\") \n  \n\n# Model for the intercepts\nlibrary(metafor)\nma1 <- rma(yi = estimate, sei = std.error, data = Intercepts)\nsummary(ma1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nRandom-Effects Model (k = 35; tau^2 estimator: REML)\n\n   logLik   deviance        AIC        BIC       AICc   \n-274.9958   549.9916   553.9916   557.0444   554.3787   \n\ntau^2 (estimated amount of total heterogeneity): 607939.3750 (SE = 150941.5658)\ntau (square root of estimated tau^2 value):      779.7047\nI^2 (total heterogeneity / total variability):   98.88%\nH^2 (total variability / sampling variability):  89.38\n\nTest for Heterogeneity:\nQ(df = 34) = 3402.9633, p-val < .0001\n\nModel Results:\n\n estimate        se     zval    pval      ci.lb      ci.ub      \n3479.3087  133.3611  26.0894  <.0001  3217.9258  3740.6917  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\n# Model for the slopes\nma2 <- rma(yi = estimate, sei = std.error, data = Slopes)\nsummary(ma2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nRandom-Effects Model (k = 35; tau^2 estimator: REML)\n\n   logLik   deviance        AIC        BIC       AICc   \n-127.4587   254.9174   258.9174   261.9701   259.3045   \n\ntau^2 (estimated amount of total heterogeneity): 65.0917 (SE = 22.6013)\ntau (square root of estimated tau^2 value):      8.0679\nI^2 (total heterogeneity / total variability):   82.28%\nH^2 (total variability / sampling variability):  5.64\n\nTest for Heterogeneity:\nQ(df = 34) = 151.3768, p-val < .0001\n\nModel Results:\n\nestimate      se      zval    pval     ci.lb     ci.ub      \n-18.1869  1.6648  -10.9245  <.0001  -21.4499  -14.9240  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nA forest plot can be generated in {metafor}.\n\n\n::: {.cell hash='yieldloss-regression-models_cache/html/unnamed-chunk-13_443e63d0fb4714bc88ba9572462bec36'}\n\n```{.r .cell-code}\nforest(ma2)\n```\n\n::: {.cell-output-display}\n![](yieldloss-regression-models_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n### Random coefficients model\n\nHere, the `lme4` package in R offers a comprehensive suite of tools to fit linear mixed-effects models. When analyzing data from multiple trials using this package, it allows for both intercepts and slopes to vary across the trials by treating them as random effects. By doing so, the inherent assumption is that these coefficients (intercepts and slopes) are drawn from certain distributions, and the goal becomes estimating the parameters of these distributions. This modeling technique acknowledges and captures the variability present across different trials. Importantly, by treating the coefficients as random effects, `lme4` enables the sharing of information across trials. This not only provides a more holistic understanding of the underlying data structure but also enhances the precision and robustness of the coefficient estimates.\n\n\n::: {.cell hash='yieldloss-regression-models_cache/html/unnamed-chunk-14_78e831b3ca94a0dc539aecdf92560635'}\n\n```{.r .cell-code}\nlibrary(lme4)\nrc1 <- lmer(yld ~ inc + (inc |study), data = wm, \n            REML = F)\nsummary(rc1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: yld ~ inc + (inc | study)\n   Data: wm\n\n     AIC      BIC   logLik deviance df.resid \n  5319.4   5343.1  -2653.7   5307.4      376 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.7078 -0.5991 -0.0295  0.5077  3.2364 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev. Corr \n study    (Intercept) 557573.08 746.708       \n          inc             36.85   6.071  -0.29\n Residual              37228.73 192.947       \nNumber of obs: 382, groups:  study, 35\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 3455.432    128.063   26.98\ninc          -17.236      1.451  -11.88\n\nCorrelation of Fixed Effects:\n    (Intr)\ninc -0.300\noptimizer (nloptwrap) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.416806 (tol = 0.002, component 1)\n```\n:::\n\n```{.r .cell-code}\n# Extract the blups\ncc2 <- coef(rc1)$study\ncc2 %>% \n pivot_longer(1:2, names_to = \"coef\", values_to = \"value\") %>% \n ggplot(aes(x=value))+\n theme_r4pde()+\n       geom_histogram(bins = 10, fill = \"grey50\", color = \"white\")+\n       facet_wrap(~coef, scales = \"free_x\")\n```\n\n::: {.cell-output-display}\n![](yieldloss-regression-models_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nWe can produce a plot with lines for the BLUPs of the individual studies and for the overall estimate.\n\n\n::: {.cell hash='yieldloss-regression-models_cache/html/unnamed-chunk-15_5938b61917e47b80b3ab7005c35ccca9'}\n\n```{.r .cell-code}\nIntercept <- cc2$`(Intercept)`\ninc<- cc2$inc\n\ndata <- data.frame(Intercept, inc)\n\nggplot(data) +\n  geom_abline(aes(intercept = Intercept, slope = inc), colour = \"gray50\") +\n  geom_abline(aes(intercept = 3455.43, slope = -17.236), linewidth = 2)+\n  xlim(0,100)+\n  ylim(0, 6000)+\n  labs(title = \"\", x = \"Incidence (%)\", y = \"Yield (kg/ha)\") +\n  theme_r4pde()\n```\n\n::: {.cell-output-display}\n![](yieldloss-regression-models_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n### Conclusion\n\nResults from various regression methods show that the calculated damage coefficients fall within the range of -0.28 to -0.56 (see table below). This range of values represents the extent to which damage is influenced by the method in consideration. The most straightforward method, often referred to as the naive approach, produced the most conservative estimate, positioning the damage coefficient at the lower bound of the observed range. In stark contrast, computing the mean values from multiple individual regressions yielded a dc that topped the range, signifying a greater estimated impact.\n\n| Model               | intercept |  slope   | damage coefficient |\n|:--------------------|:---------:|:--------:|:------------------:|\n| Global regression   |  3299.6   |  -9.261  |       -0.28        |\n| Mean of regressions |   3482    | -19.529  |       -0.56        |\n| meta-analysis       |  3479.3   | -18.1869 |       -0.52        |\n| mixed-models        |  3455.43  | -17.236  |       -0.49        |\n\n: Table: Intercept, slope and damage coefficients for the four regression approaches to summarize the relationship between soybean yield and white mold incidence.\n\nHowever, it's worth noting that the coefficients derived from the more advanced techniques---meta-analysis and mixed-models---were quite congruent. Their close alignment suggests that both methodologies, while operating on different principles, capture the underlying dynamics of the data in somewhat analogous ways. A prominent advantage of employing these advanced techniques is their ability to encapsulate and quantify uncertainty. This capability is crucial in scientific analyses as it provides a clearer understanding of the confidence levels associated with the derived coefficients. By being able to measure and articulate this uncertainty, researchers can ensure their interpretations and subsequent decisions are founded on a solid empirical base.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}